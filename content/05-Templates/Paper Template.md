# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## ğŸ“Œ Summary
BERT introduces a masked language model and next sentence prediction for pre-training. Key innovation is bidirectional context.

## ğŸ“ƒ Citation
Devlin et al., 2018 â€“ https://arxiv.org/abs/1810.04805

## ğŸ§  Key Ideas
- Masked Language Modeling (MLM)
- Next Sentence Prediction (NSP)
- Fine-tuning on downstream tasks

## ğŸ” Whatâ€™s New?
- Bidirectionality in pretraining
- Outperformed SOTA on GLUE, SQuAD

## ğŸ’¬ Discussion
- Why MLM instead of traditional LM?
- Do we still need NSP today?

## ğŸ‘¥ Notes by Group
- [Mehrdadâ€™s Notes](../01-Topics/Transformer/Notes-Mehrdad.md)
